{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8868140c",
   "metadata": {},
   "source": [
    "<div style=\"background-color:rgb(223, 119, 160); padding: 30px; border-radius: 20px; box-shadow: 0 4px 15px rgba(255, 105, 180, 0.3); color: #F8BBD0; font-family: 'Times New Roman', serif;\">\n",
    "\n",
    "<h1 style=\"text-align: center; font-size: 38px; color: white; font-weight: bold;\">üéÄ Evaluate The Chatbots üéÄ</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d55b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from evaluate import load\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. Load model and tokenizer\n",
    "model_path = \"./interview_model\"  # Update this if different\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# 2. Load the dataset\n",
    "df = pd.read_csv(\"./Q&A_data.csv\").dropna(subset=[\"question\", \"answer\"])\n",
    "\n",
    "# 3. Clean the text\n",
    "def clean(text):\n",
    "    return text.strip().replace(\"\\n\", \" \")\n",
    "\n",
    "# 4. Load metrics\n",
    "rouge = load(\"rouge\")\n",
    "bleu = load(\"bleu\")\n",
    "\n",
    "rouge_scores = []\n",
    "bleu_scores = []\n",
    "examples = []\n",
    "\n",
    "# 5. Evaluation loop\n",
    "print(\"üîç Evaluating...\")\n",
    "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    question = clean(row[\"question\"])\n",
    "    reference = clean(row[\"answer\"])\n",
    "\n",
    "    # Prompt for inference\n",
    "    prompt = f\"<|startoftext|>\\n### Question:\\n{question}\\n\\n### Answer:\\n\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    prediction = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    prediction = prediction.split(\"### Answer:\")[-1].strip()\n",
    "\n",
    "    # Compute metrics\n",
    "    rouge_score = rouge.compute(predictions=[prediction], references=[reference])[\"rougeL\"]\n",
    "    bleu_score = bleu.compute(predictions=[prediction], references=[[reference]])[\"bleu\"]\n",
    "\n",
    "    rouge_scores.append(rouge_score)\n",
    "    bleu_scores.append(bleu_score)\n",
    "\n",
    "    # Store a few example predictions\n",
    "    if len(examples) < 5:\n",
    "        examples.append((question, reference, prediction))\n",
    "\n",
    "# 6. Print results\n",
    "print(\"\\nüìä Evaluation Results:\")\n",
    "print(f\"üî∏ Average ROUGE-L: {sum(rouge_scores)/len(rouge_scores):.4f}\")\n",
    "print(f\"üî∏ Average BLEU: {sum(bleu_scores)/len(bleu_scores):.4f}\")\n",
    "\n",
    "print(\"\\nüìù Example predictions:\")\n",
    "for q, real, pred in examples:\n",
    "    print(f\"Q: {q}\")\n",
    "    print(f\"‚úÖ Real: {real}\")\n",
    "    print(f\"ü§ñ Pred: {pred}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
